{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. N-Gram Language Models.**\n",
        "\n",
        "N-Grams are basic language models that estimate the likelihood of a subsequent word based on the previous n - 1 words. Typically, all language models, whether small, large, or advanced like transformers, predict the likelihood of a word's occurrence based on the preceding sequence of words.\n",
        "\n",
        "N-grams can be of different types, a N-gram model which look at one word to the past to predict the probability of the next word is called a bigram model. A N-Gram model which look at 2 words to the past to predict the probabiltiy of the next word is called a trigram model. So theoratically, n-gram models can look at n - 1 words to the past for predicting the next word.\n",
        "\n",
        "The reason we are not selecting the whole sequence is because of the computational challenges and the lack of better predictions. In fact, we don't need to look to the whole past words to estimate the next word, we just need a few according to **Markov Assumption**\n",
        "\n",
        "$$P(w_{1:n}) \\approx \\prod_{k=1}^{n} P(w_k \\mid w_{k-n})$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SagCB42Ys9LH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2 Bigram Language Model**\n",
        "\n",
        "Bigram language models predict the probability of a word to occur next given a single previous word, thus it is called Bigram. For example if a word 'I' is given there is a higher probability that 'am' will occur next rather than 'is' or 'has'.\n",
        "\n",
        "N-Gram including Bigram does this through **Maximum Likelihood Estimation (MLE)**. MLE is the technique of estimating parameters of a statistical model which makes the observed values more probable, ie, we are trying to align the parameters of the statistical model so that the model produced probabilites that align as closely as possible with the observed values.\n",
        "\n",
        "$$P(w_{1:n}) \\approx \\prod_{k=1}^{n} P(w_k \\mid w_{k-1})$$\n",
        "\n",
        "The MLE can be written as:\n",
        "\n",
        "$$P(w_n \\mid w_{n-1}) = \\frac{C(w_{n-1} w_n)}{\\sum_{w} C(w_{n-1} w)}$$\n",
        "\n",
        "Probability of word $w_n$ given previous word $w_{n-1}$ is equal to the count of paired words $C(w_{n - 1} w_n)$ divided by the sum of all the bigrams that share the same first word $w_{n-1}$\n",
        "\n",
        "This equation of more intutive, if the bigrams or combination of two words occur more than the the previous word and some other word, the probability will be high, else it will be low."
      ],
      "metadata": {
        "id": "WVZzpD150i9U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeR_1LJkf7Kf",
        "outputId": "ebf0324e-c869-4ce0-aeef-17670359046f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given the word 'The', the predicted next word is 'Mousterian'.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "class BigramModel:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.bigram_counts = defaultdict(Counter)\n",
        "        self.total_counts = Counter()\n",
        "\n",
        "    def train(self, corpus):\n",
        "        words = corpus.split()\n",
        "\n",
        "        for i in range(len(words) - 1):\n",
        "            current_word = words[i]\n",
        "            next_word = words[i + 1]\n",
        "\n",
        "            self.bigram_counts[current_word][next_word] += 1\n",
        "            self.total_counts[current_word] += 1\n",
        "\n",
        "    def predict(self, word):\n",
        "        if word not in self.bigram_counts:\n",
        "            return None\n",
        "\n",
        "        bigram_distribution = self.bigram_counts[word]\n",
        "        total_count = self.total_counts[word]\n",
        "        probabilities = {next_word: count / total_count for next_word, count in bigram_distribution.items()}\n",
        "        next_word = random.choices(list(probabilities.keys()), list(probabilities.values()))[0]\n",
        "\n",
        "        return next_word\n",
        "\n",
        "# Example usage:\n",
        "with open('/content/drive/MyDrive/Natural-Language-Processing/internet_archive_scifi_v3.txt', 'r') as f:\n",
        "    corpus = f.read()\n",
        "\n",
        "bigram_model = BigramModel()\n",
        "bigram_model.train(corpus)\n",
        "\n",
        "current_word = \"The\"\n",
        "predicted_next_word = bigram_model.predict(current_word)\n",
        "print(f\"Given the word '{current_word}', the predicted next word is '{predicted_next_word}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generating Text with Bigram LM**"
      ],
      "metadata": {
        "id": "vo5_Ow7-_s3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bigram_text(model, start_word, num_words):\n",
        "    generated_text = [start_word]\n",
        "    current_word = start_word\n",
        "\n",
        "    for _ in range(num_words - 1):\n",
        "        next_word = model.predict(current_word)\n",
        "        if next_word is None:\n",
        "            break\n",
        "\n",
        "        generated_text.append(next_word)\n",
        "        current_word = next_word\n",
        "\n",
        "    return ' '.join(generated_text)"
      ],
      "metadata": {
        "id": "tNw6Nget8Gtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_bigram_text(bigram_model, \"Iam\", 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_o2J3EpF-Aio",
        "outputId": "c0746df1-6c29-4265-d62a-8062805e03d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Iam eager. \"If I couldn\\'t hold off the new as new streetlamps coming of what could predict a wonder -- \" \"So our regulations. Correct?\" \"Is that some bad accent, affected if I\\'ve had been twenty thousand files I\\'ll show it would.\" I recall,\" he had given clearance from wherever his own, almost impossible. Their notable for such a poor, beat-up, bedraggled each serving. Work Estimating g acceleration and her'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3 Trigram Language Model**\n",
        "\n",
        "Trigram Language Model predicts the probability of the next word to occur given the two previous words thus it is called Trigram. Trigrams works similar to how Bigram works in case of computation, but what differ is the number of past words the Trigram attend to which is 2 and incase of bigram it is 1.\n",
        "\n",
        "Joint probability in Trigram:\n",
        "\n",
        "$$P(w_{1:n}) \\approx \\prod_{k=1}^{n} P(w_k \\mid w_{k-2}, w_{k-1})$$\n",
        "\n",
        "Same for Bigrams, we use Maximum Likelihood Estimation:\n",
        "\n",
        "$$P(w_n \\mid w_{n-2}, w_{n-1}) = \\frac{C(w_{n-2}W_{n-1} w_n)}{\\sum_{w} C(w_{n-2} w_{n - 1} w)}$$\n"
      ],
      "metadata": {
        "id": "FtMTA7Kp7OYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrigramModel:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.trigram_counts = defaultdict(Counter)\n",
        "        self.total_counts = defaultdict(Counter)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        words = corpus.split()\n",
        "        for i in range(len(words) - 2):\n",
        "            prev_word1 = words[i]\n",
        "            prev_word2 = words[i + 1]\n",
        "            next_word = words[i + 2]\n",
        "\n",
        "            context = (prev_word1, prev_word2)\n",
        "\n",
        "            self.trigram_counts[context][next_word] += 1\n",
        "            self.total_counts[prev_word1][prev_word2] += 1\n",
        "\n",
        "    def predict(self, context):\n",
        "        if context not in self.trigram_counts:\n",
        "            return None\n",
        "\n",
        "\n",
        "        trigram_distribution = self.trigram_counts[context]\n",
        "        total_count = sum(trigram_distribution.values())\n",
        "\n",
        "        probabilities = {next_word: count / total_count for next_word, count in trigram_distribution.items()}\n",
        "        next_word = random.choices(list(probabilities.keys()), list(probabilities.values()))[0]\n",
        "\n",
        "        return next_word\n",
        "\n",
        "\n",
        "trigram_model = TrigramModel()\n",
        "trigram_model.train(corpus)"
      ],
      "metadata": {
        "id": "5N8py-vP-LcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generating text using trigram LM**"
      ],
      "metadata": {
        "id": "pkl8fnlxAkfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_trigram_text(model, start_words, num_words):\n",
        "    if len(start_words) < 2:\n",
        "        raise ValueError(\"Provide at least two words to start\")\n",
        "\n",
        "    generated_text = start_words.copy()\n",
        "    context = tuple(start_words)\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        next_word = model.predict(context)\n",
        "        if next_word is None:\n",
        "            break\n",
        "\n",
        "        generated_text.append(next_word)\n",
        "        context = (context[1], next_word)\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "\n",
        "generate_trigram_text(trigram_model, [\"Oh\", \"his\"], 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "44qPV7tR_K6_",
        "outputId": "a47b9fa6-1473-4a7a-d6c7-d29ce63ee1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Oh his left hand. \"You\\'ll see a better story. And the gyros the moment and decided to outshine them.\" \"Don\\'t have to behave like solids most of those guns, Winnington, I command you to let him know, in carefully filtered screen, they seemed to have more on Palash. I returned good-naturedly. \"Heard about it just came, what would have used. We cannot risk my head?\" \"They\\'re the best all-round science-fiction writer comes along a streetcar on Earth, and here, also, the transition of power. Lon drove it, each connected by an intolerable nuisance.\" \"There\\'s no call to mind of an old man,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Perprexity For Language Model Evaluation**\n",
        "\n",
        "Perplexity is one of the metrics that can be used for evaluating language models including large language models. It measures how well a language model is able to predict the next word given previous words from a sample ot text. It gives an idea of how much the model is uncertain about the next world, if the perplexity score is low meaning that the model is performing better such that it is able to predict the next token compared to the actual test set. On the other hand a higher perplexity score indicates that the model is not confident in its prediction for a sequence of words.\n",
        "\n",
        "There is a close relation between negative cross entropy and perplexity. Perplexity is the exponentiation of the negative cross entropy of the probability of a word to occur given some previous sequences of words.\n",
        "\n",
        "Here is how we calcualte the negative cross entropy of the probability of a word to come next given previous few words,\n",
        "\n",
        "$$\\text{Loss} = -\\sum_{i=1}^{n} \\log P(w_i \\mid w_{1:i-1})$$\n",
        "\n",
        "So here we have no lables, instead what we are calculating is the probability of a word to occur given previous ones based on training and test set. When evaluating the model on a test set, the model predicts the next word in the sequence for each context. If the model predicts a word with high probability and the actual word in the test set matches this prediction, it indicates that the model is performing well. Essentially, the model's performance is measured by how closely its predicted probabilities match the actual occurrences of words in the test set.\n",
        "\n",
        "But cross entropy will not give a measure that is understandable to normal people when evaluating Language models. So here we have to convert this measure of cross entropy to something called Perplexity, it can be done simply by taking the exponent of negative cross entropy.\n",
        "\n",
        "\n",
        "$$\\text{Perplexity} = \\exp\\left(\\text{Loss} / n\\right)$$\n",
        "\n",
        "\n",
        "\n",
        "$\\text{Loss}$ is the negative cross-entropy loss calculated as\n",
        "\n",
        "- $\\sum_{i=1}^{n} \\log P(w_i \\mid w_{1:i-1})$\n",
        "- $n$ is the length of the sequence of words\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xImws_d_Dx-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(model, test_corpus, smoothing=1e-6):\n",
        "    words = test_corpus.split()\n",
        "    N = len(words) - 2  # Number of trigrams in the test corpus\n",
        "\n",
        "    if N <= 0:\n",
        "        return None  # Not enough data in test_corpus to calculate perplexity\n",
        "\n",
        "    cross_entropy_loss = 0\n",
        "\n",
        "    for i in range(N):\n",
        "        prev_word1 = words[i]\n",
        "        prev_word2 = words[i + 1]\n",
        "        actual_next_word = words[i + 2]\n",
        "\n",
        "        context = (prev_word1, prev_word2)\n",
        "\n",
        "        # Get the predicted probability of the actual next word given the context\n",
        "        if context in model.trigram_counts:\n",
        "            trigram_distribution = model.trigram_counts[context]\n",
        "            total_count = sum(trigram_distribution.values())\n",
        "            actual_word_count = trigram_distribution.get(actual_next_word, 0)\n",
        "\n",
        "            # Calculate the probability of the actual next word\n",
        "            if total_count > 0:\n",
        "                # Add smoothing value to avoid zero probability\n",
        "                probability = (actual_word_count + smoothing) / (total_count + smoothing * len(trigram_distribution))\n",
        "\n",
        "                # Accumulate the negative log likelihood\n",
        "                cross_entropy_loss -= math.log(probability)\n",
        "\n",
        "    # Calculate the average cross-entropy loss\n",
        "    cross_entropy_loss /= N\n",
        "\n",
        "    # Calculate perplexity\n",
        "    perplexity = math.exp(cross_entropy_loss)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "calculate_perplexity(trigram_model, corpus[:100000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzUfQC5PB0zu",
        "outputId": "4ded75d4-0c20-415c-974a-fe9d7c7899d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.040316634418183"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1OOS4S0V40_I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}